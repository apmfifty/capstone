{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考文献1：[Cheng Guo](https://arxiv.org/pdf/1604.06737.pdf)\n",
    "\n",
    "# 训练entity embedding模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. 准备工作\n",
    "### 0.1 调入库函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#基本计算类\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import Series,DataFrame\n",
    "\n",
    "#可视化\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display \n",
    "\n",
    "#机器学习库函数\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense,Dropout,Activation,Reshape\n",
    "from keras.layers import Merge\n",
    "from keras.layers.embeddings import Embedding\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "#时间类\n",
    "import time\n",
    "import datetime\n",
    "from isoweek import Week\n",
    "\n",
    "#文件类\n",
    "import os\n",
    "\n",
    "#其他\n",
    "import itertools\n",
    "import operator\n",
    "\n",
    "#基本设定\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 定义评估指标\n",
    "本项目采用Kaggle比赛的评估指标：RMSPE（误差百分比的均方差），可表示为\n",
    "$$\n",
    "RMSPE= \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(\\frac{y_i-\\hat{y_i}}{y_i})^2}\n",
    "$$\n",
    "其中，任何当天销售额为0的数据在评估时将被忽略； $y_i$ 表示某药店在某天的实际销售额，而$\\hat{y_i}$ 表示该药店在对应这一天的预测销售额。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3 Setting seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed=42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.4 将处理好的数据从本地硬盘读入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_train_store_raw_df='train_store_raw_df.pickle'\n",
    "file_test_store_raw_df='test_store_raw_df.pickle'\n",
    "file_feature='feature_x_list.pickle'\n",
    "path='Capstone_Project_Rossman_Sales_Prediction_1'\n",
    "\n",
    "train_store_raw_df=pd.read_pickle(os.path.join(path, file_train_store_raw_df))\n",
    "test_store_raw_df=pd.read_pickle(os.path.join(path, file_test_store_raw_df))\n",
    "feature_x_list=pd.read_pickle(os.path.join(path, file_feature)).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.5 每个特征所对应的unique数值\n",
    "- dict_feature_range：每个feature所对应的范围\n",
    "- dict_feature_offset：每个feature最小值移到0所对应的offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_x_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Store                          : unique= 1115  ,max= 1115  ,min= 1     ,range= 1115  ,offset= 1    \n",
      "DayOfWeek                      : unique= 7     ,max= 7     ,min= 1     ,range= 7     ,offset= 1    \n",
      "Year                           : unique= 3     ,max= 2015  ,min= 2013  ,range= 3     ,offset= 2013 \n",
      "Month                          : unique= 12    ,max= 12    ,min= 1     ,range= 12    ,offset= 1    \n",
      "Day                            : unique= 31    ,max= 31    ,min= 1     ,range= 31    ,offset= 1    \n",
      "DayOfYear                      : unique= 365   ,max= 365   ,min= 1     ,range= 365   ,offset= 1    \n",
      "StoreType_cat                  : unique= 4     ,max= 3     ,min= 0     ,range= 4     ,offset= 0    \n",
      "Assortment_cat                 : unique= 3     ,max= 2     ,min= 0     ,range= 3     ,offset= 0    \n",
      "StateHoliday_cat               : unique= 4     ,max= 3     ,min= 0     ,range= 4     ,offset= 0    \n",
      "SchoolHoliday                  : unique= 2     ,max= 1     ,min= 0     ,range= 2     ,offset= 0    \n",
      "Promo                          : unique= 2     ,max= 1     ,min= 0     ,range= 2     ,offset= 0    \n",
      "Promo2                         : unique= 2     ,max= 1     ,min= 0     ,range= 2     ,offset= 0    \n",
      "InPromo2Today                  : unique= 2     ,max= 1     ,min= 0     ,range= 2     ,offset= 0    \n",
      "DaysCountSinceCompetition_log  : unique= 15    ,max= 15.0  ,min= 0.0   ,range= 16    ,offset= 0    \n",
      "InCompetition                  : unique= 2     ,max= 1     ,min= 0     ,range= 2     ,offset= 0    \n",
      "InCompetitionToday             : unique= 2     ,max= 1     ,min= 0     ,range= 2     ,offset= 0    \n",
      "CompetitionDistance_log        : unique= 14    ,max= 18.0  ,min= 4.0   ,range= 15    ,offset= 4    \n",
      "DaysCountSincePromo2_log       : unique= 11    ,max= 11.0  ,min= 0.0   ,range= 12    ,offset= 0    \n"
     ]
    }
   ],
   "source": [
    "dict_feature_range={}\n",
    "dict_feature_offset={}\n",
    "\n",
    "for ii in feature_x_list:\n",
    "    unique_list=train_store_raw_df[ii].unique()\n",
    "    max_v=max(unique_list)\n",
    "    min_v=min(unique_list)\n",
    "    dict_feature_range[ii]=(int)(max_v-min_v+1)\n",
    "    dict_feature_offset[ii]=(int)(min_v)\n",
    "    print '{0: <30}'.format(ii),':','unique=','{0: <5}'.format(len(unique_list)),\\\n",
    "    ',max=','{0: <5}'.format((max_v)),',min=','{0: <5}'.format((min_v)), \\\n",
    "    ',range=','{0: <5}'.format((dict_feature_range[ii])),',offset=','{0: <5}'.format((dict_feature_offset[ii]))\n",
    "    \n",
    "\n",
    "\n",
    "#     print ii,':','unique=',len(unique_list),',max=',max_v,',min=',min_v, \\\n",
    "#     ',range=',dict_feature_range[ii],',offset=',dict_feature_offset[ii]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 准备数据\n",
    "### 1.1 将数据对应feature_x_list每列添加offset，将数据的最小值设为0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Store                          : unique= 1115  ,max= 1114  ,min= 0     ,range= 1115  ,offset= 0    \n",
      "DayOfWeek                      : unique= 7     ,max= 6     ,min= 0     ,range= 7     ,offset= 0    \n",
      "Year                           : unique= 3     ,max= 2     ,min= 0     ,range= 3     ,offset= 0    \n",
      "Month                          : unique= 12    ,max= 11    ,min= 0     ,range= 12    ,offset= 0    \n",
      "Day                            : unique= 31    ,max= 30    ,min= 0     ,range= 31    ,offset= 0    \n",
      "DayOfYear                      : unique= 365   ,max= 364   ,min= 0     ,range= 365   ,offset= 0    \n",
      "StoreType_cat                  : unique= 4     ,max= 3     ,min= 0     ,range= 4     ,offset= 0    \n",
      "Assortment_cat                 : unique= 3     ,max= 2     ,min= 0     ,range= 3     ,offset= 0    \n",
      "StateHoliday_cat               : unique= 4     ,max= 3     ,min= 0     ,range= 4     ,offset= 0    \n",
      "SchoolHoliday                  : unique= 2     ,max= 1     ,min= 0     ,range= 2     ,offset= 0    \n",
      "Promo                          : unique= 2     ,max= 1     ,min= 0     ,range= 2     ,offset= 0    \n",
      "Promo2                         : unique= 2     ,max= 1     ,min= 0     ,range= 2     ,offset= 0    \n",
      "InPromo2Today                  : unique= 2     ,max= 1     ,min= 0     ,range= 2     ,offset= 0    \n",
      "DaysCountSinceCompetition_log  : unique= 15    ,max= 15.0  ,min= 0.0   ,range= 16    ,offset= 0    \n",
      "InCompetition                  : unique= 2     ,max= 1     ,min= 0     ,range= 2     ,offset= 0    \n",
      "InCompetitionToday             : unique= 2     ,max= 1     ,min= 0     ,range= 2     ,offset= 0    \n",
      "CompetitionDistance_log        : unique= 14    ,max= 14.0  ,min= 0.0   ,range= 15    ,offset= 0    \n",
      "DaysCountSincePromo2_log       : unique= 11    ,max= 11.0  ,min= 0.0   ,range= 12    ,offset= 0    \n"
     ]
    }
   ],
   "source": [
    "modified_train_store_raw_df=train_store_raw_df.copy()\n",
    "modified_test_store_raw_df=test_store_raw_df.copy()\n",
    "for col in feature_x_list:\n",
    "    modified_train_store_raw_df[col]=modified_train_store_raw_df[col]-dict_feature_offset[col]\n",
    "    modified_test_store_raw_df[col]=modified_test_store_raw_df[col]-dict_feature_offset[col]\n",
    "    \n",
    "for ii in feature_x_list:\n",
    "    unique_list=modified_train_store_raw_df[ii].unique()\n",
    "    max_v=max(unique_list)\n",
    "    min_v=min(unique_list)\n",
    "    print '{0: <30}'.format(ii),':','unique=','{0: <5}'.format(len(unique_list)),\\\n",
    "    ',max=','{0: <5}'.format((max_v)),',min=','{0: <5}'.format((min_v)), \\\n",
    "    ',range=','{0: <5}'.format((int)(max_v-min_v+1)),',offset=','{0: <5}'.format((int)(min_v))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 构造test_df数据、train_df数据，valid_df数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mask_train=(modified_train_store_raw_df['Date']<'2015-06-15') &(modified_train_store_raw_df['Outlier_3']==False) \\\n",
    "            & (modified_train_store_raw_df['Open']==1) & (modified_train_store_raw_df['Sales']>0)\n",
    "mask_valid=(modified_train_store_raw_df['Date']>='2015-06-15') & (modified_train_store_raw_df['Open']==1)\\\n",
    "            & (modified_train_store_raw_df['Sales']>0)&(modified_train_store_raw_df['Outlier_3']==False)\n",
    "\n",
    "df_train=modified_train_store_raw_df.loc[mask_train,feature_x_list]\n",
    "df_valid=modified_train_store_raw_df.loc[mask_valid,feature_x_list]\n",
    "y_train_data=np.array(modified_train_store_raw_df.loc[mask_train,'Sales'])\n",
    "y_valid_data=np.array(modified_train_store_raw_df.loc[mask_valid,'Sales'])\n",
    "\n",
    "df_test=modified_test_store_raw_df.loc[modified_test_store_raw_df['Open']==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 验证test数据,valid数据的feature是否在train数据中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Store                          = 1\n",
      "DayOfWeek                      = 1\n",
      "Year                           = 1\n",
      "Month                          = 1\n",
      "Day                            = 1\n",
      "DayOfYear                      = 1\n",
      "StoreType_cat                  = 1\n",
      "Assortment_cat                 = 1\n",
      "StateHoliday_cat               = 1\n",
      "SchoolHoliday                  = 1\n",
      "Promo                          = 1\n",
      "Promo2                         = 1\n",
      "InPromo2Today                  = 1\n",
      "DaysCountSinceCompetition_log  = 1\n",
      "InCompetition                  = 1\n",
      "InCompetitionToday             = 1\n",
      "CompetitionDistance_log        = 1\n",
      "DaysCountSincePromo2_log       = 1\n"
     ]
    }
   ],
   "source": [
    "for ii in feature_x_list:\n",
    "    set_unique_train=set(df_train[ii].unique())\n",
    "    set_unique_valid=set(df_valid[ii].unique())\n",
    "    set_unique_test=set(df_test[ii].unique())\n",
    "    \n",
    "    if set_unique_train.issuperset(set_unique_valid) & set_unique_train.issuperset(set_unique_test):\n",
    "        print '{0: <30}'.format(ii),'= 1'\n",
    "    else:\n",
    "        print '{0: <30}'.format(ii),'= 0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 搭建Embedding模型，并预测结果\n",
    "### 2.1 构造embedding模型所需要的train, valid数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train_data=[]\n",
    "x_valid_data=[]\n",
    "for ii in feature_x_list:\n",
    "    x_train_data.append(np.array(df_train[ii]))\n",
    "    x_valid_data.append(np.array(df_valid[ii]))  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 搭建embedding模型\n",
    "- 构造list models，将每个feature对应的embedding装进去\n",
    "    - 输入range: dict_feature_range\n",
    "    - 对应的embedding数目:calc_embedding_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calc_embedding_space(x):\n",
    "    dict_map={\n",
    "        1115:50,\n",
    "        365:30,\n",
    "        31:10,\n",
    "        16:10,\n",
    "        15:10,\n",
    "        12:8,\n",
    "        7:6,\n",
    "        4:3,        \n",
    "        3:2\n",
    "    }\n",
    "    return dict_map[x]\n",
    "\n",
    "\n",
    "models=[]\n",
    "for ii in feature_x_list:\n",
    "    input_range=dict_feature_range[ii]\n",
    "    \n",
    "    if input_range ==2:\n",
    "        temp_name='Dense_'+ii\n",
    "        model= Sequential(name=temp_name)\n",
    "        model.add(Dense(1, input_dim=1))\n",
    "        models.append(model)\n",
    "    else:    \n",
    "        embedding_space=calc_embedding_space(input_range)        \n",
    "        temp_name='Embedding_'+ii\n",
    "        model=Sequential(name=temp_name)\n",
    "        model.add(Embedding(input_range,embedding_space,input_length=1))\n",
    "        model.add(Reshape((embedding_space, ), input_shape=(1,embedding_space)))\n",
    "        models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_embedding_model():\n",
    "    dropout_rate=0.1\n",
    "\n",
    "    embedding_model=Sequential()\n",
    "    embedding_model.add(Merge(models,mode='concat',concat_axis=-1))\n",
    "\n",
    "\n",
    "    embedding_model.add(Dense(512,kernel_initializer='uniform'))\n",
    "    embedding_model.add(Activation('relu'))\n",
    "    embedding_model.add(Dense(128,kernel_initializer='uniform'))\n",
    "    embedding_model.add(Activation('relu'))\n",
    "    embedding_model.add(Dense(32,kernel_initializer='uniform'))\n",
    "    embedding_model.add(Activation('relu'))\n",
    "    embedding_model.add(Dropout(dropout_rate))\n",
    "    embedding_model.add(Dense(1))\n",
    "    embedding_model.add(Activation('sigmoid'))\n",
    "\n",
    "    embedding_model.compile(loss='mean_absolute_error',optimizer='adam')\n",
    "    return embedding_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2.3 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 786180 samples, validate on 45177 samples\n",
      "Epoch 1/1\n",
      "786180/786180 [==============================] - 36s - loss: 0.0148 - val_loss: 0.0094\n",
      "1epoch_10drop_embedding_3Layers_run.csv------Done\n",
      "Train on 786180 samples, validate on 45177 samples\n",
      "Epoch 1/2\n",
      "786180/786180 [==============================] - 40s - loss: 0.0151 - val_loss: 0.0101\n",
      "Epoch 2/2\n",
      "786180/786180 [==============================] - 38s - loss: 0.0081 - val_loss: 0.0100\n",
      "2epoch_10drop_embedding_3Layers_run.csv------Done\n",
      "Train on 786180 samples, validate on 45177 samples\n",
      "Epoch 1/3\n",
      "786180/786180 [==============================] - 39s - loss: 0.0148 - val_loss: 0.0099\n",
      "Epoch 2/3\n",
      "786180/786180 [==============================] - 41s - loss: 0.0081 - val_loss: 0.0098\n",
      "Epoch 3/3\n",
      "786180/786180 [==============================] - 37s - loss: 0.0069 - val_loss: 0.0099\n",
      "3epoch_10drop_embedding_3Layers_run.csv------Done\n",
      "Train on 786180 samples, validate on 45177 samples\n",
      "Epoch 1/4\n",
      "786180/786180 [==============================] - 39s - loss: 0.0152 - val_loss: 0.0109\n",
      "Epoch 2/4\n",
      "786180/786180 [==============================] - 37s - loss: 0.0080 - val_loss: 0.0105\n",
      "Epoch 3/4\n",
      "786180/786180 [==============================] - 37s - loss: 0.0069 - val_loss: 0.0105\n",
      "Epoch 4/4\n",
      "786180/786180 [==============================] - 35s - loss: 0.0067 - val_loss: 0.0103\n",
      "4epoch_10drop_embedding_3Layers_run.csv------Done\n",
      "Train on 786180 samples, validate on 45177 samples\n",
      "Epoch 1/5\n",
      "786180/786180 [==============================] - 39s - loss: 0.0138 - val_loss: 0.0096\n",
      "Epoch 2/5\n",
      "786180/786180 [==============================] - 37s - loss: 0.0080 - val_loss: 0.0099\n",
      "Epoch 3/5\n",
      "786180/786180 [==============================] - 36s - loss: 0.0067 - val_loss: 0.0109\n",
      "Epoch 4/5\n",
      "786180/786180 [==============================] - 37s - loss: 0.0065 - val_loss: 0.0100\n",
      "Epoch 5/5\n",
      "786180/786180 [==============================] - 37s - loss: 0.0064 - val_loss: 0.0104\n",
      "5epoch_10drop_embedding_3Layers_run.csv------Done\n",
      "Train on 786180 samples, validate on 45177 samples\n",
      "Epoch 1/6\n",
      "786180/786180 [==============================] - 39s - loss: 0.0154 - val_loss: 0.0096\n",
      "Epoch 2/6\n",
      "786180/786180 [==============================] - 39s - loss: 0.0078 - val_loss: 0.0102\n",
      "Epoch 3/6\n",
      "786180/786180 [==============================] - 40s - loss: 0.0069 - val_loss: 0.0104\n",
      "Epoch 4/6\n",
      "786180/786180 [==============================] - 43s - loss: 0.0067 - val_loss: 0.0108\n",
      "Epoch 5/6\n",
      "786180/786180 [==============================] - 44s - loss: 0.0066 - val_loss: 0.0102\n",
      "Epoch 6/6\n",
      "786180/786180 [==============================] - 44s - loss: 0.0065 - val_loss: 0.0100\n",
      "6epoch_10drop_embedding_3Layers_run.csv------Done\n",
      "Train on 786180 samples, validate on 45177 samples\n",
      "Epoch 1/7\n",
      "786180/786180 [==============================] - 39s - loss: 0.0140 - val_loss: 0.0163\n",
      "Epoch 2/7\n",
      "786180/786180 [==============================] - 37s - loss: 0.0082 - val_loss: 0.0102\n",
      "Epoch 3/7\n",
      "786180/786180 [==============================] - 38s - loss: 0.0075 - val_loss: 0.0102\n",
      "Epoch 4/7\n",
      "786180/786180 [==============================] - 38s - loss: 0.0067 - val_loss: 0.0114\n",
      "Epoch 5/7\n",
      "786180/786180 [==============================] - 36s - loss: 0.0065 - val_loss: 0.0105\n",
      "Epoch 6/7\n",
      "786180/786180 [==============================] - 36s - loss: 0.0064 - val_loss: 0.0105\n",
      "Epoch 7/7\n",
      "786180/786180 [==============================] - 35s - loss: 0.0063 - val_loss: 0.0100\n",
      "7epoch_10drop_embedding_3Layers_run.csv------Done\n",
      "Train on 786180 samples, validate on 45177 samples\n",
      "Epoch 1/8\n",
      "786180/786180 [==============================] - 41s - loss: 0.0154 - val_loss: 0.0101\n",
      "Epoch 2/8\n",
      "786180/786180 [==============================] - 45s - loss: 0.0076 - val_loss: 0.0103\n",
      "Epoch 3/8\n",
      "786180/786180 [==============================] - 40s - loss: 0.0066 - val_loss: 0.0102\n",
      "Epoch 4/8\n",
      "786180/786180 [==============================] - 40s - loss: 0.0064 - val_loss: 0.0110\n",
      "Epoch 5/8\n",
      "786180/786180 [==============================] - 40s - loss: 0.0063 - val_loss: 0.0109\n",
      "Epoch 6/8\n",
      "786180/786180 [==============================] - 40s - loss: 0.0063 - val_loss: 0.0108\n",
      "Epoch 7/8\n",
      "786180/786180 [==============================] - 39s - loss: 0.0062 - val_loss: 0.0107\n",
      "Epoch 8/8\n",
      "786180/786180 [==============================] - 36s - loss: 0.0062 - val_loss: 0.0102\n",
      "8epoch_10drop_embedding_3Layers_run.csv------Done\n",
      "Train on 786180 samples, validate on 45177 samples\n",
      "Epoch 1/9\n",
      "786180/786180 [==============================] - 40s - loss: 0.0143 - val_loss: 0.0105\n",
      "Epoch 2/9\n",
      "786180/786180 [==============================] - 39s - loss: 0.0076 - val_loss: 0.0107\n",
      "Epoch 3/9\n",
      "786180/786180 [==============================] - 37s - loss: 0.0065 - val_loss: 0.0117\n",
      "Epoch 4/9\n",
      "786180/786180 [==============================] - 36s - loss: 0.0063 - val_loss: 0.0101\n",
      "Epoch 5/9\n",
      "786180/786180 [==============================] - 36s - loss: 0.0063 - val_loss: 0.0100\n",
      "Epoch 6/9\n",
      "786180/786180 [==============================] - 36s - loss: 0.0062 - val_loss: 0.0099\n",
      "Epoch 7/9\n",
      "786180/786180 [==============================] - 37s - loss: 0.0061 - val_loss: 0.0120\n",
      "Epoch 8/9\n",
      "786180/786180 [==============================] - 37s - loss: 0.0061 - val_loss: 0.0100\n",
      "Epoch 9/9\n",
      "786180/786180 [==============================] - 37s - loss: 0.0060 - val_loss: 0.0104\n",
      "9epoch_10drop_embedding_3Layers_run.csv------Done\n",
      "Train on 786180 samples, validate on 45177 samples\n",
      "Epoch 1/10\n",
      "786180/786180 [==============================] - 41s - loss: 0.0153 - val_loss: 0.0103\n",
      "Epoch 2/10\n",
      "786180/786180 [==============================] - 39s - loss: 0.0077 - val_loss: 0.0110\n",
      "Epoch 3/10\n",
      "786180/786180 [==============================] - 37s - loss: 0.0066 - val_loss: 0.0104\n",
      "Epoch 4/10\n",
      "786180/786180 [==============================] - 38s - loss: 0.0064 - val_loss: 0.0110\n",
      "Epoch 5/10\n",
      "786180/786180 [==============================] - 37s - loss: 0.0063 - val_loss: 0.0114\n",
      "Epoch 6/10\n",
      "786180/786180 [==============================] - 37s - loss: 0.0062 - val_loss: 0.0105\n",
      "Epoch 7/10\n",
      "786180/786180 [==============================] - 38s - loss: 0.0061 - val_loss: 0.0120\n",
      "Epoch 8/10\n",
      "786180/786180 [==============================] - 37s - loss: 0.0061 - val_loss: 0.0110\n",
      "Epoch 9/10\n",
      "786180/786180 [==============================] - 37s - loss: 0.0060 - val_loss: 0.0118\n",
      "Epoch 10/10\n",
      "786180/786180 [==============================] - 37s - loss: 0.0060 - val_loss: 0.0117\n",
      "10epoch_10drop_embedding_3Layers_run.csv------Done\n"
     ]
    }
   ],
   "source": [
    "max_log_y=np.max(np.log(y_train_data))\n",
    "\n",
    "def _val_for_fit(val):\n",
    "    val=np.log(val)\n",
    "    return val/max_log_y\n",
    "def _val_for_pred(val):\n",
    "    return np.exp(val*max_log_y)\n",
    "\n",
    "nb_epoch=10\n",
    "for epoch_run in range(1,int(nb_epoch+1)):\n",
    "    embedding_model=create_embedding_model()\n",
    "    embedding_model.fit(x_train_data, _val_for_fit(y_train_data), \n",
    "                    validation_data=(x_valid_data,_val_for_fit(y_valid_data)),\n",
    "#                     epochs=nb_epoch, \n",
    "                     epochs=epoch_run,\n",
    "                        batch_size=256\n",
    "                           # callbacks=[self.checkpointer],\n",
    "                )\n",
    "\n",
    "    \n",
    "    modified_test_store_raw_df.loc[modified_test_store_raw_df.Open==0,'Sales']=0\n",
    "    modified_test_store_raw_df['tuple_map']=modified_test_store_raw_df[feature_x_list].apply(tuple,axis=1)\n",
    "\n",
    "    \n",
    "    for index, row in modified_test_store_raw_df[modified_test_store_raw_df.Open==1].iterrows():\n",
    "        input_value=(list(modified_test_store_raw_df.loc[index,'tuple_map']))\n",
    "        t=[np.array([x]) for x in input_value]    \n",
    "        modified_test_store_raw_df.loc[index,'Sales']=_val_for_pred(embedding_model.predict(t))[0][0]\n",
    "\n",
    "    test_output=modified_test_store_raw_df[['Id','Sales']].copy()\n",
    "    test_output.sort_values(by='Id',inplace=True)\n",
    "    path='Capstone_Project_Rossman_Sales_Prediction_1'\n",
    "    filename='{}epoch_{}drop_embedding_3Layers_run.csv'.format(epoch_run,int(dropout_rate*100))\n",
    "\n",
    "    test_output.to_csv(os.path.join(path, filename),index=False)\n",
    "    print filename+'------Done'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 在valid数据上评估模型RMSPE结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 用训练好的模型预测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 118.676782846 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "modified_test_store_raw_df.loc[modified_test_store_raw_df.Open==0,'Sales']=0\n",
    "modified_test_store_raw_df['tuple_map']=modified_test_store_raw_df[feature_x_list].apply(tuple,axis=1)\n",
    "\n",
    "for index, row in modified_test_store_raw_df[modified_test_store_raw_df.Open==1].iterrows():\n",
    "    input_value=(list(modified_test_store_raw_df.loc[index,'tuple_map']))\n",
    "    t=[np.array([x]) for x in input_value]    \n",
    "    modified_test_store_raw_df.loc[index,'Sales']=_val_for_pred(embedding_model.predict(t))[0][0]\n",
    "\n",
    "test_output=modified_test_store_raw_df[['Id','Sales']].copy()\n",
    "test_output.sort_values(by='Id',inplace=True)\n",
    "path='Capstone_Project_Rossman_Sales_Prediction_1'\n",
    "filename='{}epoch_{}drop_embedding_3Layers_result.csv'.format(nb_epoch,int(dropout_rate*100))\n",
    "\n",
    "test_output.to_csv(os.path.join(path, filename),index=False)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 训练XGBoost模型\n",
    "Refers to [XGBoost Feature Importance](https://www.kaggle.com/cast42/rossmann-store-sales/xgboost-in-python-with-rmspe-v2)\n",
    "\n",
    "Based on https://www.kaggle.com/justdoit/rossmann-store-sales/xgboost-in-python-with-rmspe/code\n",
    "\n",
    "Public Score :  0.11389\n",
    "\n",
    "Private Validation Score :  0.096959"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 构建XGBoost模型，并预测结果\n",
    "### 2.1 构造XGBoost模型所需要的train, valid数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train,X_valid=df_train,df_valid\n",
    "y_train=modified_train_store_raw_df.loc[mask_train,'SalesLog']\n",
    "y_valid=modified_train_store_raw_df.loc[mask_valid,'SalesLog']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 设置XGBoost模型的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data processed\n"
     ]
    }
   ],
   "source": [
    "print('training data processed')\n",
    "params={\n",
    "    'objective':'reg:linear',\n",
    "    'booster':'gbtree',\n",
    "    'eta':0.3,\n",
    "    'max_depth':10,\n",
    "    'subsample':0.9,\n",
    "    'colsample_bytree':0.7,\n",
    "    'silent':1,\n",
    "    'seed':seed\n",
    "}\n",
    "num_boost_round=100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until eval error hasn't decreased in 100 rounds.\n",
      "[0]\ttrain-rmspe:0.996818\teval-rmspe:0.996931\n",
      "[1]\ttrain-rmspe:0.981394\teval-rmspe:0.982063\n",
      "[2]\ttrain-rmspe:0.937594\teval-rmspe:0.939122\n",
      "[3]\ttrain-rmspe:0.855673\teval-rmspe:0.857887\n",
      "[4]\ttrain-rmspe:0.742694\teval-rmspe:0.745082\n",
      "[5]\ttrain-rmspe:0.617960\teval-rmspe:0.618971\n",
      "[6]\ttrain-rmspe:0.503472\teval-rmspe:0.501832\n",
      "[7]\ttrain-rmspe:0.412915\teval-rmspe:0.408229\n",
      "[8]\ttrain-rmspe:0.348143\teval-rmspe:0.340473\n",
      "[9]\ttrain-rmspe:0.313813\teval-rmspe:0.303335\n",
      "[10]\ttrain-rmspe:0.290919\teval-rmspe:0.276755\n",
      "[11]\ttrain-rmspe:0.274935\teval-rmspe:0.258841\n",
      "[12]\ttrain-rmspe:0.273709\teval-rmspe:0.257537\n",
      "[13]\ttrain-rmspe:0.274886\teval-rmspe:0.257788\n",
      "[14]\ttrain-rmspe:0.275724\teval-rmspe:0.258048\n",
      "[15]\ttrain-rmspe:0.277641\teval-rmspe:0.259475\n",
      "[16]\ttrain-rmspe:0.278100\teval-rmspe:0.261810\n",
      "[17]\ttrain-rmspe:0.263861\teval-rmspe:0.247302\n",
      "[18]\ttrain-rmspe:0.259885\teval-rmspe:0.244582\n",
      "[19]\ttrain-rmspe:0.256477\teval-rmspe:0.242986\n",
      "[20]\ttrain-rmspe:0.255374\teval-rmspe:0.240592\n",
      "[21]\ttrain-rmspe:0.249339\teval-rmspe:0.234139\n",
      "[22]\ttrain-rmspe:0.248794\teval-rmspe:0.234085\n",
      "[23]\ttrain-rmspe:0.244851\teval-rmspe:0.230740\n",
      "[24]\ttrain-rmspe:0.242496\teval-rmspe:0.228300\n",
      "[25]\ttrain-rmspe:0.241806\teval-rmspe:0.228552\n",
      "[26]\ttrain-rmspe:0.233430\teval-rmspe:0.220131\n",
      "[27]\ttrain-rmspe:0.231498\teval-rmspe:0.218203\n",
      "[28]\ttrain-rmspe:0.231130\teval-rmspe:0.218204\n",
      "[29]\ttrain-rmspe:0.218206\teval-rmspe:0.206290\n",
      "[30]\ttrain-rmspe:0.212583\teval-rmspe:0.201444\n",
      "[31]\ttrain-rmspe:0.209159\teval-rmspe:0.197929\n",
      "[32]\ttrain-rmspe:0.208444\teval-rmspe:0.197399\n",
      "[33]\ttrain-rmspe:0.207088\teval-rmspe:0.196422\n",
      "[34]\ttrain-rmspe:0.205525\teval-rmspe:0.195313\n",
      "[35]\ttrain-rmspe:0.203626\teval-rmspe:0.193346\n",
      "[36]\ttrain-rmspe:0.197381\teval-rmspe:0.187651\n",
      "[37]\ttrain-rmspe:0.196993\teval-rmspe:0.187652\n",
      "[38]\ttrain-rmspe:0.196220\teval-rmspe:0.186983\n",
      "[39]\ttrain-rmspe:0.192916\teval-rmspe:0.183308\n",
      "[40]\ttrain-rmspe:0.190677\teval-rmspe:0.180687\n",
      "[41]\ttrain-rmspe:0.189411\teval-rmspe:0.179836\n",
      "[42]\ttrain-rmspe:0.186994\teval-rmspe:0.176606\n",
      "[43]\ttrain-rmspe:0.186303\teval-rmspe:0.176732\n",
      "[44]\ttrain-rmspe:0.182689\teval-rmspe:0.172827\n",
      "[45]\ttrain-rmspe:0.181829\teval-rmspe:0.171751\n",
      "[46]\ttrain-rmspe:0.181518\teval-rmspe:0.171482\n",
      "[47]\ttrain-rmspe:0.181181\teval-rmspe:0.171155\n",
      "[48]\ttrain-rmspe:0.179602\teval-rmspe:0.169426\n",
      "[49]\ttrain-rmspe:0.177534\teval-rmspe:0.167028\n",
      "[50]\ttrain-rmspe:0.176108\teval-rmspe:0.165605\n",
      "[51]\ttrain-rmspe:0.174292\teval-rmspe:0.163759\n",
      "[52]\ttrain-rmspe:0.172954\teval-rmspe:0.161359\n",
      "[53]\ttrain-rmspe:0.172748\teval-rmspe:0.161209\n",
      "[54]\ttrain-rmspe:0.171402\teval-rmspe:0.159844\n",
      "[55]\ttrain-rmspe:0.170538\teval-rmspe:0.159119\n",
      "[56]\ttrain-rmspe:0.170308\teval-rmspe:0.159154\n",
      "[57]\ttrain-rmspe:0.169852\teval-rmspe:0.158709\n",
      "[58]\ttrain-rmspe:0.168350\teval-rmspe:0.157410\n",
      "[59]\ttrain-rmspe:0.167688\teval-rmspe:0.157102\n",
      "[60]\ttrain-rmspe:0.166204\teval-rmspe:0.155383\n",
      "[61]\ttrain-rmspe:0.165342\teval-rmspe:0.154280\n",
      "[62]\ttrain-rmspe:0.164732\teval-rmspe:0.154183\n",
      "[63]\ttrain-rmspe:0.164387\teval-rmspe:0.153689\n",
      "[64]\ttrain-rmspe:0.164258\teval-rmspe:0.153624\n",
      "[65]\ttrain-rmspe:0.163618\teval-rmspe:0.152819\n",
      "[66]\ttrain-rmspe:0.162997\teval-rmspe:0.153511\n",
      "[67]\ttrain-rmspe:0.162556\teval-rmspe:0.153625\n",
      "[68]\ttrain-rmspe:0.161361\teval-rmspe:0.152513\n",
      "[69]\ttrain-rmspe:0.160972\teval-rmspe:0.152675\n",
      "[70]\ttrain-rmspe:0.158085\teval-rmspe:0.151351\n",
      "[71]\ttrain-rmspe:0.157868\teval-rmspe:0.151455\n",
      "[72]\ttrain-rmspe:0.157606\teval-rmspe:0.151327\n",
      "[73]\ttrain-rmspe:0.157410\teval-rmspe:0.151156\n",
      "[74]\ttrain-rmspe:0.156610\teval-rmspe:0.150391\n",
      "[75]\ttrain-rmspe:0.155668\teval-rmspe:0.149008\n",
      "[76]\ttrain-rmspe:0.154945\teval-rmspe:0.148430\n",
      "[77]\ttrain-rmspe:0.154840\teval-rmspe:0.148374\n",
      "[78]\ttrain-rmspe:0.154638\teval-rmspe:0.148066\n",
      "[79]\ttrain-rmspe:0.154000\teval-rmspe:0.147198\n",
      "[80]\ttrain-rmspe:0.153436\teval-rmspe:0.146858\n",
      "[81]\ttrain-rmspe:0.152978\teval-rmspe:0.146084\n",
      "[82]\ttrain-rmspe:0.152313\teval-rmspe:0.145386\n",
      "[83]\ttrain-rmspe:0.151564\teval-rmspe:0.144895\n",
      "[84]\ttrain-rmspe:0.151189\teval-rmspe:0.144608\n",
      "[85]\ttrain-rmspe:0.150959\teval-rmspe:0.144552\n",
      "[86]\ttrain-rmspe:0.150621\teval-rmspe:0.144411\n",
      "[87]\ttrain-rmspe:0.149561\teval-rmspe:0.143435\n",
      "[88]\ttrain-rmspe:0.149292\teval-rmspe:0.143205\n",
      "[89]\ttrain-rmspe:0.148374\teval-rmspe:0.142442\n",
      "[90]\ttrain-rmspe:0.147998\teval-rmspe:0.142032\n",
      "[91]\ttrain-rmspe:0.147535\teval-rmspe:0.141534\n",
      "[92]\ttrain-rmspe:0.147358\teval-rmspe:0.141394\n",
      "[93]\ttrain-rmspe:0.146613\teval-rmspe:0.140770\n",
      "[94]\ttrain-rmspe:0.146066\teval-rmspe:0.140191\n",
      "[95]\ttrain-rmspe:0.145685\teval-rmspe:0.139829\n",
      "[96]\ttrain-rmspe:0.145180\teval-rmspe:0.139640\n",
      "[97]\ttrain-rmspe:0.144755\teval-rmspe:0.139622\n",
      "[98]\ttrain-rmspe:0.144166\teval-rmspe:0.138980\n",
      "[99]\ttrain-rmspe:0.144093\teval-rmspe:0.138964\n"
     ]
    }
   ],
   "source": [
    "dtrain=xgb.DMatrix(X_train[feature_x_list],y_train)\n",
    "dvalid=xgb.DMatrix(X_valid[feature_x_list],y_valid)\n",
    "\n",
    "def rmspe(y,yhat):\n",
    "    return np.sqrt(np.mean((yhat/y-1)**2))\n",
    "def rmspe_xg(yhat,y):\n",
    "    y=np.expm1(y.get_label())\n",
    "    yhat=np.expm1(yhat)\n",
    "    return 'rmspe',rmspe(y,yhat)\n",
    "\n",
    "watchlist=[(dtrain,'train'),(dvalid,'eval')]\n",
    "gbm=xgb.train(params,dtrain,num_boost_round,evals=watchlist,early_stopping_rounds=100,feval=rmspe_xg,verbose_eval=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 利用训练好的模型进行预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating\n",
      "RMSPE:813.055656\n"
     ]
    }
   ],
   "source": [
    "print('Validating')\n",
    "yhat=gbm.predict(xgb.DMatrix(X_valid[feature_x_list]))\n",
    "error=rmspe(y_valid,np.expm1(yhat))\n",
    "print('RMSPE:{:.6f}'.format(error))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
